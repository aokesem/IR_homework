# RAG系统配置文件

# 模型配置
models:
  # Embedding模型
  embedding:
    name: "BAAI/bge-small-zh-v1.5"  
    device: "cuda" 
  
  # Reranker模型 (重排序)
  reranker:
    name: "BAAI/bge-reranker-large"
    device: "cuda"
    enable: true
  
  # LLM模型
  llm:
    provider: "huggingface"
    
    # HuggingFace 本地模型配置
    name: "Qwen/Qwen2-0.5B-Instruct" 
    device: "cuda" 
    max_new_tokens: 512
    temperature: 0.7
    load_in_4bit: false
    
    # Ollama 配置
    ollama:
      base_url: "http://localhost:11434"
      model: "qwen2.5:7b"      # 默认 Ollama 模型
      temperature: 0.7

  # 可选模型列表 (用于前端选择)
  available_models:
    - name: "Qwen/Qwen2-0.5B-Instruct"
      provider: "huggingface"
      desc: "本地显存模型 (0.5B)"
    - name: "qwen2.5:7b"
      provider: "ollama"
      desc: "Ollama (Qwen 2.5 7B)"
    - name: "llama3:8b"
      provider: "ollama"
      desc: "Ollama (Llama 3 8B)"
    - name: "mistral"
      provider: "ollama"
      desc: "Ollama (Mistral)"

# 文档处理配置
document:
  chunk_size: 512  # 每个文档块的token数
  chunk_overlap: 50  # 块之间重叠的token数
  
# 检索配置
retrieval:
  top_k: 5  # 检索top5个最相关文档
  score_threshold: 0.3  # 相似度阈值

# 数据路径
paths:
  data_dir: "./data"
  raw_docs: "./data/raw"
  processed_docs: "./data/processed"
  vector_store: "./vector_store"
  models_cache: "./models"
  conversations: "./data/conversations"

# Web界面配置
web:
  host: "127.0.0.1"
  port: 7860
  share: false  
  
# 评测配置 (Ragas)
evaluation:
  api:
    base_url: "https://tb.api.mkeai.com/v1"
    api_key: "sk-ipf4pyozQpL905IkkcEe0nycy1r1xEr0oYSAgpf7IW9o0F6b"
    model: "deepseek-r1-0528"
