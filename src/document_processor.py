"""
æ–‡æ¡£å¤„ç†æ¨¡å—
è´Ÿè´£åŠ è½½ã€æ¸…æ´—ã€åˆ‡åˆ†æ–‡æ¡£åŠä¸Šä¸‹æ–‡å¢å¼º
"""
import os
import json
import time
from typing import List, Dict, Optional
from pathlib import Path

from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    # ä¸Šä¸‹æ–‡å¢å¼º Prompt
    CONTEXT_PROMPT = """è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ä¸Šä¸‹æ–‡è¯´æ˜ï¼ˆ20-30å­—ä»¥å†…ï¼‰ã€‚
è¯´æ˜è¯¥ç‰‡æ®µæ¥è‡ªæ–‡ä»¶ã€Š{filename}ã€‹ï¼Œå¹¶æ¦‚æ‹¬å…¶æ ¸å¿ƒå†…å®¹ã€‚
æ ¼å¼è¦æ±‚ï¼š[å…³äº{filename}çš„...è¯´æ˜]

æ–‡æœ¬ç‰‡æ®µï¼š
{chunk_content}

ä¸Šä¸‹æ–‡è¯´æ˜ï¼š"""

    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50, processed_dir: str = None):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processed_dir = Path(processed_dir) if processed_dir else None
        if self.processed_dir:
            self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""],
            length_function=len,
        )
        
        self.loader_mapping = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.md': UnstructuredMarkdownLoader,
        }
    
    def load_documents(self, file_paths: List[str]) -> List[Document]:
        documents = []
        for file_path in file_paths:
            try:
                docs = self._load_single_file(file_path)
                documents.extend(docs)
                print(f"âœ“ æˆåŠŸåŠ è½½: {file_path} ({len(docs)} ä¸ªæ–‡æ¡£)")
            except Exception as e:
                print(f"âœ— åŠ è½½å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        return documents
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.loader_mapping:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        loader_class = self.loader_mapping[file_ext]
        if file_ext == '.txt':
            loader = loader_class(file_path, encoding='utf-8')
        else:
            loader = loader_class(file_path)
        
        documents = loader.load()
        for doc in documents:
            doc.metadata['source'] = file_path
            doc.metadata['file_name'] = os.path.basename(file_path)
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        chunks = self.text_splitter.split_documents(documents)
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i
        print(f"æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªæ–‡æ¡£å—")
        return chunks
    
    def clean_text(self, text: str) -> str:
        text = ' '.join(text.split())
        return text.strip()

    def augment_chunk_with_context(self, chunk: Document, generator) -> Document:
        """
        ä½¿ç”¨ LLM ä¸º Chunk ç”Ÿæˆä¸Šä¸‹æ–‡å‰ç¼€
        """
        if not generator:
            return chunk
            
        filename = chunk.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
        content = chunk.page_content
        
        # æ„é€  Prompt
        prompt = self.CONTEXT_PROMPT.format(
            filename=filename,
            chunk_content=content[:500]  # é™åˆ¶é•¿åº¦ä»¥èŠ‚çœToken
        )
        
        try:
            # è°ƒç”¨ç”Ÿæˆå™¨ (ä½¿ç”¨ç®€å•çš„ç”Ÿæˆæ¨¡å¼ï¼Œä¸åšRAG)
            # è¿™é‡Œæˆ‘ä»¬éœ€è¦ç›´æ¥è®¿é—®åº•å±‚æ¥å£æˆ–è€…ä½¿ç”¨ä¸€ä¸ªç‰¹å®šçš„æ–¹æ³•
            # å‡è®¾ generator æœ‰ _generate_hf æˆ–ç±»ä¼¼çš„ç›´æ¥ç”Ÿæˆèƒ½åŠ›
            # ä¸ºäº†é€šç”¨ï¼Œæˆ‘ä»¬æ„é€ ä¸€ä¸ª dummy history è°ƒç”¨ generate
            
            result = generator.generate(
                question=prompt,
                context_documents=[], # ç©ºä¸Šä¸‹æ–‡
                history=[],
                custom_prompt="{question}" # ç›´æ¥é€ä¼ 
            )
            
            context_desc = result['answer'].strip()
            # æ¸…ç†å¯èƒ½çš„æ‹¬å·
            context_desc = context_desc.replace("ä¸Šä¸‹æ–‡è¯´æ˜ï¼š", "").strip()
            
            # æ‹¼æ¥åˆ°åŸå§‹å†…å®¹å‰é¢
            new_content = f"{context_desc}\n{content}"
            chunk.page_content = new_content
            chunk.metadata['is_augmented'] = True
            
            return chunk
            
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡å¢å¼ºå¤±è´¥: {e}")
            return chunk

    def _get_cache_path(self, file_path: str) -> Path:
        if not self.processed_dir: return None
        return self.processed_dir / f"{os.path.basename(file_path)}.json"

    def _save_cache(self, file_path: str, chunks: List[Document]):
        """ä¿å­˜å¤„ç†åçš„Chunksåˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(file_path)
        if not cache_path: return
        try:
            mtime = os.path.getmtime(file_path)
            cache_data = {
                "file_path": file_path,
                "mtime": mtime,
                "chunks": [ # æ³¨æ„è¿™é‡Œæ”¹ä¸ºä¿å­˜ chunks
                    {"page_content": c.page_content, "metadata": c.metadata}
                    for c in chunks
                ]
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è­¦å‘Š: å†™å…¥ç¼“å­˜å¤±è´¥ {file_path}: {e}")

    def _load_cache(self, file_path: str) -> List[Document]:
        cache_path = self._get_cache_path(file_path)
        if not cache_path or not cache_path.exists(): return None
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if data.get("mtime") != os.path.getmtime(file_path): return None
            # æ³¨æ„è¿™é‡ŒåŠ è½½çš„æ˜¯ chunks
            if "chunks" in data:
                 return [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in data["chunks"]]
            return None # æ—§ç‰ˆæœ¬ç¼“å­˜ä¸å…¼å®¹
        except Exception:
            return None

    def process_directory(self, directory: str, generator=None) -> List[Document]:
        """
        å¤„ç†ç›®å½•ï¼Œæ”¯æŒä¼ å…¥ generator è¿›è¡Œå¢å¼º
        """
        file_paths = []
        for ext in self.loader_mapping.keys():
            file_paths.extend(Path(directory).glob(f'**/*{ext}'))
        file_paths = [str(fp) for fp in file_paths]
        
        if not file_paths:
            print(f"è­¦å‘Š: {directory} ä¸ºç©º")
            return []
        
        final_chunks = []
        
        for fp in file_paths:
            # 1. å°è¯•ä»ç¼“å­˜åŠ è½½ (æ­¤æ—¶ç¼“å­˜é‡Œå·²ç»æ˜¯åˆ‡åˆ†ä¸”å¢å¼ºè¿‡çš„ chunks)
            cached_chunks = self._load_cache(fp)
            if cached_chunks:
                final_chunks.extend(cached_chunks)
                print(f"ğŸš€ ä»ç¼“å­˜åŠ è½½Chunks: {os.path.basename(fp)}")
                continue
            
            # 2. å¦‚æœæ— ç¼“å­˜ï¼Œåˆ™é‡æ–°å¤„ç†
            try:
                # Load
                docs = self._load_single_file(fp)
                for doc in docs: doc.page_content = self.clean_text(doc.page_content)
                
                # Split
                file_chunks = self.split_documents(docs)
                
                # Augment (å¦‚æœæä¾›äº†ç”Ÿæˆå™¨)
                if generator:
                    print(f"ğŸ¤– æ­£åœ¨å¢å¼º {len(file_chunks)} ä¸ªåˆ‡ç‰‡ (æ­¤è¿‡ç¨‹è¾ƒæ…¢)...")
                    augmented_chunks = []
                    for chunk in file_chunks:
                        aug_chunk = self.augment_chunk_with_context(chunk, generator)
                        augmented_chunks.append(aug_chunk)
                        print(".", end="", flush=True)
                    print(" å®Œæˆ!")
                    file_chunks = augmented_chunks
                
                # Save chunks to cache
                self._save_cache(fp, file_chunks)
                final_chunks.extend(file_chunks)
                
            except Exception as e:
                print(f"âœ— å¤„ç†å¤±è´¥: {fp}, {e}")
                
        return final_chunks

if __name__ == "__main__":
    pass