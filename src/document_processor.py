"""
æ–‡æ¡£å¤„ç†æ¨¡å—
è´Ÿè´£åŠ è½½ã€æ¸…æ´—å’Œåˆ‡åˆ†æ–‡æ¡£
"""
import os
import json
import time
from typing import List, Dict
from pathlib import Path

from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document




class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    def __init__(self, chunk_size: int = 512, chunk_overlap: int = 50, processed_dir: str = None):
        """
            chunk_size: æ¯ä¸ªæ–‡æ¡£å—çš„å­—ç¬¦æ•°
            chunk_overlap: å—ä¹‹é—´é‡å çš„å­—ç¬¦æ•°
            processed_dir: ç¼“å­˜å·²è§£ææ–‡æ¡£çš„ç›®å½•
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processed_dir = Path(processed_dir) if processed_dir else None
        if self.processed_dir:
            self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–æ–‡æœ¬åˆ‡åˆ†å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""],
            length_function=len,
        )
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼æ˜ å°„
        self.loader_mapping = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.md': UnstructuredMarkdownLoader,
        }
    
    def load_documents(self, file_paths: List[str]) -> List[Document]:
        """
        Args:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        
        for file_path in file_paths:
            try:
                docs = self._load_single_file(file_path)
                documents.extend(docs)
                print(f"âœ“ æˆåŠŸåŠ è½½: {file_path} ({len(docs)} ä¸ªæ–‡æ¡£)")
            except Exception as e:
                print(f"âœ— åŠ è½½å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        
        return documents
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        """
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        file_ext = Path(file_path).suffix.lower()
        
        if file_ext not in self.loader_mapping:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        loader_class = self.loader_mapping[file_ext]
        
        # ç‰¹æ®Šå¤„ç†ï¼šPDFå’Œæ–‡æœ¬æ–‡ä»¶ä½¿ç”¨UTF-8ç¼–ç 
        if file_ext == '.txt':
            loader = loader_class(file_path, encoding='utf-8')
        else:
            loader = loader_class(file_path)
        
        documents = loader.load()
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£æ·»åŠ å…ƒæ•°æ®
        for doc in documents:
            doc.metadata['source'] = file_path
            doc.metadata['file_name'] = os.path.basename(file_path)
        
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            åˆ‡åˆ†åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        chunks = self.text_splitter.split_documents(documents)
        
        # ä¸ºæ¯ä¸ªchunkæ·»åŠ ç´¢å¼•
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i
        
        print(f"æ–‡æ¡£åˆ‡åˆ†å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£ -> {len(chunks)} ä¸ªæ–‡æ¡£å—")
        
        return chunks
    
    def _get_cache_path(self, file_path: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        if not self.processed_dir:
            return None
        # ä½¿ç”¨åŸæ–‡ä»¶å + .json ä½œä¸ºç¼“å­˜æ‰©å±•å
        rel_path = os.path.basename(file_path)
        return self.processed_dir / f"{rel_path}.json"

    def _save_cache(self, file_path: str, documents: List[Document]):
        """å°†è§£æåçš„æ–‡æ¡£ä¿å­˜åˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(file_path)
        if not cache_path:
            return
            
        try:
            # è®°å½•åŸå§‹å†…å®¹å’Œä¿®æ”¹æ—¶é—´ä»¥è¿›è¡Œæ ¡éªŒ
            mtime = os.path.getmtime(file_path)
            cache_data = {
                "file_path": file_path,
                "mtime": mtime,
                "documents": [
                    {"page_content": doc.page_content, "metadata": doc.metadata}
                    for doc in documents
                ]
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è­¦å‘Š: å†™å…¥ç¼“å­˜å¤±è´¥ {file_path}: {e}")

    def _load_cache(self, file_path: str) -> List[Document]:
        """ä»ç¼“å­˜åŠ è½½æ–‡æ¡£"""
        cache_path = self._get_cache_path(file_path)
        if not cache_path or not cache_path.exists():
            return None
            
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # æ ¡éªŒæ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹è¿‡
            if data.get("mtime") != os.path.getmtime(file_path):
                return None
                
            documents = [
                Document(page_content=d["page_content"], metadata=d["metadata"])
                for d in data["documents"]
            ]
            return documents
        except Exception:
            return None
    
    def clean_text(self, text: str) -> str:
        """
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            æ¸…æ´—åçš„æ–‡æœ¬
        """
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = ' '.join(text.split())
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆæ ¹æ®éœ€è¦è°ƒæ•´ï¼‰
        # text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)
        
        return text.strip()
    
    def process_directory(self, directory: str) -> List[Document]:
        """
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            å¤„ç†åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        # æ”¶é›†æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶
        file_paths = []
        for ext in self.loader_mapping.keys():
            file_paths.extend(Path(directory).glob(f'**/*{ext}'))
        
        file_paths = [str(fp) for fp in file_paths]
        
        if not file_paths:
            print(f"è­¦å‘Š: åœ¨ {directory} ä¸­æœªæ‰¾åˆ°æ”¯æŒçš„æ–‡æ¡£")
            return []
        
        print(f"æ‰¾åˆ° {len(file_paths)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        # åŠ è½½æ–‡æ¡£ï¼ˆå¸¦æœ‰ç¼“å­˜é€»è¾‘ï¼‰
        documents = []
        for fp in file_paths:
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            cached_docs = self._load_cache(fp)
            if cached_docs:
                documents.extend(cached_docs)
                print(f"ğŸš€ ä»ç¼“å­˜åŠ è½½: {os.path.basename(fp)}")
            else:
                # æ­£å¸¸è§£æ
                try:
                    docs = self._load_single_file(fp)
                    # æ¸…æ´—æ–‡æœ¬
                    for doc in docs:
                        doc.page_content = self.clean_text(doc.page_content)
                    
                    documents.extend(docs)
                    # å†™å…¥ç¼“å­˜
                    self._save_cache(fp, docs)
                    print(f"ğŸ“‚ è§£ææ–°æ–‡ä»¶: {os.path.basename(fp)}")
                except Exception as e:
                    print(f"âœ— å¤„ç†å¤±è´¥: {fp}, é”™è¯¯: {str(e)}")
        
        # ç»Ÿä¸€è¿›è¡Œæ–‡æ¡£åˆ‡åˆ†
        chunks = self.split_documents(documents)
        
        return chunks


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    processor = DocumentProcessor(chunk_size=512, chunk_overlap=50)
    
    # æµ‹è¯•å¤„ç†ç›®å½•
    chunks = processor.process_directory("../data/raw")
    
    if chunks:
        print(f"\nç¤ºä¾‹æ–‡æ¡£å—:")
        print(f"å†…å®¹é¢„è§ˆ: {chunks[0].page_content[:200]}...")
        print(f"å…ƒæ•°æ®: {chunks[0].metadata}")
