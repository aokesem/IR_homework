"""
æ–‡æ¡£å¤„ç†æ¨¡å—
è´Ÿè´£åŠ è½½ã€æ¸…æ´—ã€åˆ‡åˆ†æ–‡æ¡£åŠä¸Šä¸‹æ–‡å¢å¼º
æ”¯æŒ: è¯­ä¹‰åˆ‡åˆ† (Semantic Chunking)
æ”¯æŒ: å¤šç‰ˆæœ¬å…±å­˜ (A/B Test)
"""
import os
import json
import time
from typing import List, Dict, Optional
from pathlib import Path

from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# å°è¯•å¯¼å…¥è¯­ä¹‰åˆ‡åˆ†å™¨
try:
    from langchain_experimental.text_splitter import SemanticChunker
    HAS_SEMANTIC_CHUNKER = True
except ImportError:
    HAS_SEMANTIC_CHUNKER = False
    print("âš ï¸ æœªæ‰¾åˆ° langchain_experimentalï¼Œè¯­ä¹‰åˆ‡åˆ†ä¸å¯ç”¨")

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    CONTEXT_PROMPT = """è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ä¸Šä¸‹æ–‡è¯´æ˜ï¼ˆ20-30å­—ä»¥å†…ï¼‰ã€‚
è¯´æ˜è¯¥ç‰‡æ®µæ¥è‡ªæ–‡ä»¶ã€Š{filename}ã€‹ï¼Œå¹¶æ¦‚æ‹¬å…¶æ ¸å¿ƒå†…å®¹ã€‚
æ ¼å¼è¦æ±‚ï¼š[å…³äº{filename}çš„...è¯´æ˜]

æ–‡æœ¬ç‰‡æ®µï¼š
{chunk_content}

ä¸Šä¸‹æ–‡è¯´æ˜ï¼š"""

    def __init__(
        self, 
        chunk_size: int = 512, 
        chunk_overlap: int = 50, 
        processed_dir: str = None,
        use_semantic_chunking: bool = False,
        embeddings = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processed_dir = Path(processed_dir) if processed_dir else None
        self.default_use_semantic = use_semantic_chunking # ä¿å­˜é»˜è®¤è®¾ç½®
        self.embeddings = embeddings
        
        if self.processed_dir:
            self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. åŸºç¡€åˆ‡åˆ†å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""],
            length_function=len,
        )
        
        # 2. è¯­ä¹‰åˆ‡åˆ†å™¨ (é¢„åˆå§‹åŒ–)
        self.semantic_splitter = None
        if HAS_SEMANTIC_CHUNKER and embeddings:
            try:
                self.semantic_splitter = SemanticChunker(
                    embeddings,
                    breakpoint_threshold_type="percentile",
                    breakpoint_threshold_amount=90
                )
            except Exception as e:
                print(f"âŒ åˆå§‹åŒ–è¯­ä¹‰åˆ‡åˆ†å™¨å¤±è´¥: {e}")
        
        self.loader_mapping = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.md': UnstructuredMarkdownLoader,
        }
    
    def load_documents(self, file_paths: List[str]) -> List[Document]:
        documents = []
        for file_path in file_paths:
            try:
                docs = self._load_single_file(file_path)
                documents.extend(docs)
            except Exception as e:
                print(f"âœ— åŠ è½½å¤±è´¥: {file_path}, {e}")
        return documents
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.loader_mapping:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        loader_class = self.loader_mapping[file_ext]
        kwargs = {'encoding': 'utf-8'} if file_ext == '.txt' else {}
        loader = loader_class(file_path, **kwargs)
        
        documents = loader.load()
        for doc in documents:
            doc.metadata['source'] = file_path
            doc.metadata['file_name'] = os.path.basename(file_path)
        return documents
    
    def split_documents(self, documents: List[Document], use_semantic: bool = False) -> List[Document]:
        """
        åˆ‡åˆ†æ–‡æ¡£
        Args:
            use_semantic: æ˜¯å¦å¼ºåˆ¶ä½¿ç”¨è¯­ä¹‰åˆ‡åˆ†
        """
        # åªæœ‰å½“å¯ç”¨äº†è¯­ä¹‰åˆ‡åˆ†ï¼Œä¸”ç¯å¢ƒæ”¯æŒï¼Œä¸”ä¼ å…¥äº†embeddingsæ—¶æ‰æ‰§è¡Œ
        if use_semantic and self.semantic_splitter:
            # print("  - æ‰§è¡Œè¯­ä¹‰åˆ‡åˆ†...")
            try:
                chunks = self.semantic_splitter.split_documents(documents)
                # å…œåº•è¶…é•¿å—
                final_chunks = []
                for chunk in chunks:
                    if len(chunk.page_content) > self.chunk_size * 1.5:
                        sub_chunks = self.text_splitter.split_documents([chunk])
                        final_chunks.extend(sub_chunks)
                    else:
                        final_chunks.append(chunk)
                chunks = final_chunks
            except Exception as e:
                print(f"âš ï¸ è¯­ä¹‰åˆ‡åˆ†å¤±è´¥ ({e})ï¼Œå›é€€åˆ°åŸºç¡€åˆ‡åˆ†")
                chunks = self.text_splitter.split_documents(documents)
        else:
            # print("  - æ‰§è¡ŒåŸºç¡€åˆ‡åˆ†...")
            chunks = self.text_splitter.split_documents(documents)
        
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i
        
        return chunks
    
    def clean_text(self, text: str) -> str:
        text = ' '.join(text.split())
        return text.strip()

    def augment_chunk_with_context(self, chunk: Document, generator) -> Document:
        if not generator: return chunk
        filename = chunk.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
        content = chunk.page_content
        prompt = self.CONTEXT_PROMPT.format(filename=filename, chunk_content=content[:500])
        try:
            result = generator.generate(question=prompt, context_documents=[], history=[], custom_prompt="{question}")
            context_desc = result['answer'].strip().replace("ä¸Šä¸‹æ–‡è¯´æ˜ï¼š", "").strip()
            chunk.page_content = f"{context_desc}\n{content}"
            chunk.metadata['is_augmented'] = True
            return chunk
        except Exception:
            return chunk

    def _get_cache_path(self, file_path: str, label: str = "") -> Path:
        """ç¼“å­˜è·¯å¾„åŒ…å«labelï¼Œé˜²æ­¢å†²çª"""
        if not self.processed_dir: return None
        # label æ¸…ç†æ–‡ä»¶åéæ³•å­—ç¬¦
        safe_label = label.replace("[", "").replace("]", "").replace(" ", "_")
        filename = f"{os.path.basename(file_path)}_{safe_label}.json"
        return self.processed_dir / filename

    def _save_cache(self, file_path: str, chunks: List[Document], label: str = ""):
        cache_path = self._get_cache_path(file_path, label)
        if not cache_path: return
        try:
            mtime = os.path.getmtime(file_path)
            cache_data = {
                "file_path": file_path,
                "mtime": mtime,
                "chunks": [{"page_content": c.page_content, "metadata": c.metadata} for c in chunks]
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è­¦å‘Š: å†™å…¥ç¼“å­˜å¤±è´¥ {file_path}: {e}")

    def _load_cache(self, file_path: str, label: str = "") -> List[Document]:
        cache_path = self._get_cache_path(file_path, label)
        if not cache_path or not cache_path.exists(): return None
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if data.get("mtime") != os.path.getmtime(file_path): return None
            if "chunks" in data:
                 return [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in data["chunks"]]
            return None
        except Exception:
            return None

    def process_directory(
        self, 
        directory: str, 
        generator=None, 
        label: str = "", 
        enable_semantic: bool = False,
        enable_augmentation: bool = False
    ) -> List[Document]:
        """
        å¤„ç†ç›®å½•
        Args:
            label: é™„åŠ åˆ°æ–‡ä»¶åçš„æ ‡ç­¾ (e.g. " [åŸºç¡€ç‰ˆ]")
            enable_semantic: æ˜¯å¦å¯ç”¨è¯­ä¹‰åˆ‡åˆ†
            enable_augmentation: æ˜¯å¦å¯ç”¨LLMå¢å¼º
        """
        file_paths = []
        for ext in self.loader_mapping.keys():
            file_paths.extend(Path(directory).glob(f'**/*{ext}'))
        file_paths = [str(fp) for fp in file_paths]
        
        if not file_paths:
            return []
        
        final_chunks = []
        print(f"\n>> å¼€å§‹å¤„ç†ä»»åŠ¡: {label} (è¯­ä¹‰åˆ‡åˆ†={enable_semantic}, å¢å¼º={enable_augmentation})")
        
        for fp in file_paths:
            # 1. å°è¯•ä»ç¼“å­˜åŠ è½½ (å¸¦Label)
            cached_chunks = self._load_cache(fp, label)
            if cached_chunks:
                final_chunks.extend(cached_chunks)
                # print(f"ğŸš€ [ç¼“å­˜] {os.path.basename(fp)}{label}")
                continue
            
            try:
                # Load
                docs = self._load_single_file(fp)
                for doc in docs: doc.page_content = self.clean_text(doc.page_content)
                
                # Split (ä¼ å…¥ç‰¹å®šçš„ç­–ç•¥)
                file_chunks = self.split_documents(docs, use_semantic=enable_semantic)
                
                # Augment (ä»…å½“å¯ç”¨å¢å¼ºä¸”ä¼ å…¥äº†ç”Ÿæˆå™¨æ—¶)
                if enable_augmentation and generator:
                    print(f"ğŸ¤– [å¢å¼º] {os.path.basename(fp)} ({len(file_chunks)} chunks)...")
                    augmented_chunks = []
                    for chunk in file_chunks:
                        aug_chunk = self.augment_chunk_with_context(chunk, generator)
                        augmented_chunks.append(aug_chunk)
                        print(".", end="", flush=True)
                    print(" Done")
                    file_chunks = augmented_chunks
                
                # Tagging: ä¿®æ”¹æ–‡ä»¶åå…ƒæ•°æ®ï¼ŒåŠ ä¸Š Label
                for chunk in file_chunks:
                    chunk.metadata['file_name'] = chunk.metadata['file_name'] + label
                
                # Save chunks to cache (å¸¦Label)
                self._save_cache(fp, file_chunks, label)
                final_chunks.extend(file_chunks)
                print(f"âœ… [å®Œæˆ] {os.path.basename(fp)}{label}")
                
            except Exception as e:
                print(f"âœ— å¤±è´¥: {fp}, {e}")
                
        return final_chunks

if __name__ == "__main__":
    pass