"""
æ–‡æ¡£å¤„ç†æ¨¡å—
è´Ÿè´£åŠ è½½ã€æ¸…æ´—ã€åˆ‡åˆ†æ–‡æ¡£åŠä¸Šä¸‹æ–‡å¢å¼º
æ”¯æŒ: è¯­ä¹‰åˆ‡åˆ† (Semantic Chunking)
"""
import os
import json
import time
from typing import List, Dict, Optional
from pathlib import Path

from langchain_community.document_loaders import (
    TextLoader,
    PyPDFLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# å°è¯•å¯¼å…¥è¯­ä¹‰åˆ‡åˆ†å™¨
try:
    from langchain_experimental.text_splitter import SemanticChunker
    HAS_SEMANTIC_CHUNKER = True
except ImportError:
    HAS_SEMANTIC_CHUNKER = False
    print("âš ï¸ æœªæ‰¾åˆ° langchain_experimentalï¼Œè¯­ä¹‰åˆ‡åˆ†ä¸å¯ç”¨")

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""
    
    # ä¸Šä¸‹æ–‡å¢å¼º Prompt
    CONTEXT_PROMPT = """è¯·ä¸ºä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„ä¸Šä¸‹æ–‡è¯´æ˜ï¼ˆ20-30å­—ä»¥å†…ï¼‰ã€‚
è¯´æ˜è¯¥ç‰‡æ®µæ¥è‡ªæ–‡ä»¶ã€Š{filename}ã€‹ï¼Œå¹¶æ¦‚æ‹¬å…¶æ ¸å¿ƒå†…å®¹ã€‚
æ ¼å¼è¦æ±‚ï¼š[å…³äº{filename}çš„...è¯´æ˜]

æ–‡æœ¬ç‰‡æ®µï¼š
{chunk_content}

ä¸Šä¸‹æ–‡è¯´æ˜ï¼š"""

    def __init__(
        self, 
        chunk_size: int = 512, 
        chunk_overlap: int = 50, 
        processed_dir: str = None,
        use_semantic_chunking: bool = False,
        embeddings = None
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.processed_dir = Path(processed_dir) if processed_dir else None
        self.use_semantic_chunking = use_semantic_chunking
        self.embeddings = embeddings
        
        if self.processed_dir:
            self.processed_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. åŸºç¡€åˆ‡åˆ†å™¨
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", ".", "!", "?", ";", " ", ""],
            length_function=len,
        )
        
        # 2. è¯­ä¹‰åˆ‡åˆ†å™¨
        self.semantic_splitter = None
        if use_semantic_chunking and HAS_SEMANTIC_CHUNKER and embeddings:
            print("ğŸš€ å¯ç”¨è¯­ä¹‰åˆ‡åˆ† (Semantic Chunking)")
            try:
                # ä½¿ç”¨ç™¾åˆ†ä½é˜ˆå€¼ç­–ç•¥
                self.semantic_splitter = SemanticChunker(
                    embeddings,
                    breakpoint_threshold_type="percentile",
                    breakpoint_threshold_amount=90 # é˜ˆå€¼è¶Šé«˜åˆ‡å¾—è¶Šç¢
                )
            except Exception as e:
                print(f"âŒ åˆå§‹åŒ–è¯­ä¹‰åˆ‡åˆ†å™¨å¤±è´¥: {e}")
        
        self.loader_mapping = {
            '.txt': TextLoader,
            '.pdf': PyPDFLoader,
            '.docx': Docx2txtLoader,
            '.md': UnstructuredMarkdownLoader,
        }
    
    def load_documents(self, file_paths: List[str]) -> List[Document]:
        documents = []
        for file_path in file_paths:
            try:
                docs = self._load_single_file(file_path)
                documents.extend(docs)
                print(f"âœ“ æˆåŠŸåŠ è½½: {file_path} ({len(docs)} ä¸ªæ–‡æ¡£)")
            except Exception as e:
                print(f"âœ— åŠ è½½å¤±è´¥: {file_path}, é”™è¯¯: {str(e)}")
        return documents
    
    def _load_single_file(self, file_path: str) -> List[Document]:
        file_ext = Path(file_path).suffix.lower()
        if file_ext not in self.loader_mapping:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}")
        
        loader_class = self.loader_mapping[file_ext]
        if file_ext == '.txt':
            loader = loader_class(file_path, encoding='utf-8')
        else:
            loader = loader_class(file_path)
        
        documents = loader.load()
        for doc in documents:
            doc.metadata['source'] = file_path
            doc.metadata['file_name'] = os.path.basename(file_path)
        return documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ‡åˆ†æ–‡æ¡£"""
        # å¦‚æœå¯ç”¨äº†è¯­ä¹‰åˆ‡åˆ†ä¸”åˆå§‹åŒ–æˆåŠŸï¼Œä¼˜å…ˆä½¿ç”¨è¯­ä¹‰åˆ‡åˆ†
        if self.semantic_splitter:
            print("æ­£åœ¨è¿›è¡Œè¯­ä¹‰åˆ‡åˆ†...")
            try:
                chunks = self.semantic_splitter.split_documents(documents)
                print(f"è¯­ä¹‰åˆ‡åˆ†å®Œæˆ: {len(documents)} -> {len(chunks)} Chunks")
                
                # è¯­ä¹‰åˆ‡åˆ†åå¯èƒ½å‡ºç°è¶…å¤§å—ï¼Œå†æ¬¡ç”¨å­—ç¬¦åˆ‡åˆ†å™¨å…œåº•å¤„ç†ä¸€ä¸‹è¶…é•¿å—
                final_chunks = []
                for chunk in chunks:
                    if len(chunk.page_content) > self.chunk_size * 1.5:
                        sub_chunks = self.text_splitter.split_documents([chunk])
                        final_chunks.extend(sub_chunks)
                    else:
                        final_chunks.append(chunk)
                chunks = final_chunks
                
            except Exception as e:
                print(f"âš ï¸ è¯­ä¹‰åˆ‡åˆ†å¤±è´¥ ({e})ï¼Œå›é€€åˆ°åŸºç¡€åˆ‡åˆ†")
                chunks = self.text_splitter.split_documents(documents)
        else:
            chunks = self.text_splitter.split_documents(documents)
        
        for i, chunk in enumerate(chunks):
            chunk.metadata['chunk_id'] = i
        
        if not self.semantic_splitter:
             print(f"åŸºç¡€åˆ‡åˆ†å®Œæˆ: {len(documents)} -> {len(chunks)} Chunks")
             
        return chunks
    
    def clean_text(self, text: str) -> str:
        text = ' '.join(text.split())
        return text.strip()

    def augment_chunk_with_context(self, chunk: Document, generator) -> Document:
        """ä½¿ç”¨ LLM ä¸º Chunk ç”Ÿæˆä¸Šä¸‹æ–‡å‰ç¼€"""
        if not generator: return chunk
        filename = chunk.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
        content = chunk.page_content
        
        prompt = self.CONTEXT_PROMPT.format(
            filename=filename,
            chunk_content=content[:500]
        )
        
        try:
            result = generator.generate(
                question=prompt,
                context_documents=[],
                history=[],
                custom_prompt="{question}"
            )
            context_desc = result['answer'].strip().replace("ä¸Šä¸‹æ–‡è¯´æ˜ï¼š", "").strip()
            chunk.page_content = f"{context_desc}\n{content}"
            chunk.metadata['is_augmented'] = True
            return chunk
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡å¢å¼ºå¤±è´¥: {e}")
            return chunk

    def _get_cache_path(self, file_path: str) -> Path:
        if not self.processed_dir: return None
        return self.processed_dir / f"{os.path.basename(file_path)}.json"

    def _save_cache(self, file_path: str, chunks: List[Document]):
        """ä¿å­˜å¤„ç†åçš„Chunksåˆ°ç¼“å­˜"""
        cache_path = self._get_cache_path(file_path)
        if not cache_path: return
        try:
            mtime = os.path.getmtime(file_path)
            cache_data = {
                "file_path": file_path,
                "mtime": mtime,
                "chunks": [{"page_content": c.page_content, "metadata": c.metadata} for c in chunks]
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"è­¦å‘Š: å†™å…¥ç¼“å­˜å¤±è´¥ {file_path}: {e}")

    def _load_cache(self, file_path: str) -> List[Document]:
        cache_path = self._get_cache_path(file_path)
        if not cache_path or not cache_path.exists(): return None
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if data.get("mtime") != os.path.getmtime(file_path): return None
            if "chunks" in data:
                 return [Document(page_content=d["page_content"], metadata=d["metadata"]) for d in data["chunks"]]
            return None
        except Exception:
            return None

    def process_directory(self, directory: str, generator=None) -> List[Document]:
        """å¤„ç†ç›®å½•"""
        file_paths = []
        for ext in self.loader_mapping.keys():
            file_paths.extend(Path(directory).glob(f'**/*{ext}'))
        file_paths = [str(fp) for fp in file_paths]
        
        if not file_paths:
            print(f"è­¦å‘Š: {directory} ä¸ºç©º")
            return []
        
        final_chunks = []
        for fp in file_paths:
            cached_chunks = self._load_cache(fp)
            if cached_chunks:
                final_chunks.extend(cached_chunks)
                print(f"ğŸš€ ä»ç¼“å­˜åŠ è½½Chunks: {os.path.basename(fp)}")
                continue
            
            try:
                docs = self._load_single_file(fp)
                for doc in docs: doc.page_content = self.clean_text(doc.page_content)
                file_chunks = self.split_documents(docs)
                
                if generator:
                    print(f"ğŸ¤– æ­£åœ¨å¢å¼º {len(file_chunks)} ä¸ªåˆ‡ç‰‡ (æ­¤è¿‡ç¨‹è¾ƒæ…¢)...")
                    augmented_chunks = []
                    for chunk in file_chunks:
                        aug_chunk = self.augment_chunk_with_context(chunk, generator)
                        augmented_chunks.append(aug_chunk)
                        print(".", end="", flush=True)
                    print(" å®Œæˆ!")
                    file_chunks = augmented_chunks
                
                self._save_cache(fp, file_chunks)
                final_chunks.extend(file_chunks)
            except Exception as e:
                print(f"âœ— å¤„ç†å¤±è´¥: {fp}, {e}")
                
        return final_chunks

if __name__ == "__main__":
    pass
