1. 文档切分 (Chunking) —— 
document_processor.py
实现方式：使用的是 RecursiveCharacterTextSplitter（递归字符切分器）。

逻辑：它不是简单地按字数切断，而是尝试按照一个层级列表（["\n\n", "\n", "。", "！", "？", "；", ".", "!", "?", ";", " ", ""]）递归地寻找最佳切分点。
参数：
chunk_size: 512 字符（每个块的最大长度）。
chunk_overlap: 50 字符（相邻块之间的重叠，保证语义连贯，防止信息在切分处断裂）。
评价：这是目前最通用的方案，能较好地保留段落和句子的完整性。

2. 向量嵌入 (Embedding) —— 
retriever.py
实现方式：调用 HuggingFaceEmbeddings 加载本地模型。

模型：BAAI/bge-small-zh-v1.5。这是目前中文领域公认效果最好的小尺寸嵌入模型之一。
处理：使用了 normalize_embeddings=True。这意味着所有的向量都被归一化为单位向量，这样在计算相似度时，点积（Dot Product）直接等同于余弦相似度（Cosine Similarity）。
原理：将每一段 512 字符的文本变成一个 512 维（bge-small 的维度）的浮点数向量。

3. 文档检索 (Retrieval) —— 
retriever.py
实现方式：使用 FAISS (Facebook AI Similarity Search) 向量数据库。

算法：默认使用的是 IndexFlatL2（暴力搜索）或受 LangChain 封装影响的近似索引。它计算查询向量与库中所有文档向量的距离。
流程：
将用户的提问（Query）通过同样的 BGE 模型转化成向量。
在 FAISS 中搜索最接近的 top_k（默认 5）个文本块。
过滤掉得分低于 score_threshold（默认 0.3）的低相关块。

4. 答案生成 (Generation) —— 
generator.py
实现方式：加载本地 Qwen (通义千问) 系列模型（如 Qwen2-0.5B-Instruct）。

Prompt 构造：这是 RAG 的精髓。我们将检索到的 5 个文本块拼接成 
context
，填入以下模板：
text
基于以下已知信息，简洁和专业地回答用户的问题。
如果无法从中得到答案，请说 “根据已知信息无法回答该问题”。
已知信息：
{context}
用户问题：
{question}
推理：模型根据这个“带着参考书”的 Prompt，生成最终的自然语言回答。