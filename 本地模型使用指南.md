# 使用本地模型运行指南

## ✅ 已完成配置

### 模型配置
- **LLM模型**: Qwen/Qwen3-0.6B (本地缓存)
- **模型路径**: C:\Users\aokesem\.cache\huggingface\hub\models--Qwen--Qwen3-0.6B
- **Embedding模型**: BAAI/bge-small-zh-v1.5 (首次会自动下载~400MB)

### 代码优化
✅ 已修改 `generator.py`，兼容基础模型和Instruct模型

## 🚀 启动方式

### 方法1: 双击启动脚本
双击 `start.bat` 文件即可

### 方法2: 命令行
```bash
cd "d:\Python Code\信息检索原理作业"
D:/Anaconda3/envs/LLM_env/python.exe app.py
```

## 📝 首次使用流程

1. **启动应用**
   - 运行 start.bat 或执行 python app.py
   - 系统会加载本地的 Qwen3-0.6B 模型
   - BGE embedding 模型会自动下载（约400MB，仅首次）

2. **构建知识库**
   - 在浏览器打开 http://127.0.0.1:7860
   - 进入"📁 文档管理"标签页
   - 点击"🔨 从data/raw目录构建知识库"
   - 等待处理完成（已有演示文档）

3. **开始问答**
   - 进入"💬 问答"标签页
   - 输入问题，例如：
     * "什么是信息检索？"
     * "RAG技术的优势是什么？"
     * "向量检索和传统检索的区别？"

## 💡 注意事项

### 关于Qwen3-0.6B基础模型
- 这是**基础版**模型，不是Instruct版
- 生成的答案可能格式不如Instruct版规范
- 如果效果不理想，可以考虑：
  1. 使用本地的 Qwen2-0.5B-Instruct（在config.yaml中切换）
  2. 下载其他Instruct模型

### 如何切换模型
编辑 `config.yaml` 文件，修改这一行：
```yaml
name: "Qwen/Qwen3-0.6B"  # 改为其他模型名称
```

本地可用的模型：
- `Qwen/Qwen3-0.6B` (当前使用)
- `Qwen/Qwen2-0.5B-Instruct` (建议试试这个，Instruct版本)
- `Qwen/Qwen3-0.6B-Base`

## 🔧 故障排除

**Q: 模型加载失败？**
检查路径: `C:\Users\aokesem\.cache\huggingface\hub\models--Qwen--Qwen3-0.6B`

**Q: 回答质量不好？**
尝试切换到 Qwen2-0.5B-Instruct：
```yaml
name: "Qwen/Qwen2-0.5B-Instruct"
```

**Q: 显存不足？**
在 config.yaml 中设置：
```yaml
device: "cpu"
load_in_4bit: false
```

## 📊 预期效果

使用Qwen3-0.6B基础模型：
- ✅ 速度快
- ✅ 显存占用低 (~2GB)
- ⚠️ 回答格式可能不够规范
- ⚠️ 需要好的prompt引导

建议切换到Qwen2-0.5B-Instruct获得更好体验！
